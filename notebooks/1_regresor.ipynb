{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresor logístico\n",
    "\n",
    "Notebook escrito por Multivacs en 2025.  \n",
    "Como referencia [logistic.ipynb](https://colab.research.google.com/github/jaspock/me/blob/main/docs/materials/transformers/assets/notebooks/logistic.ipynb) de Juan Antonio Pérez.\n",
    "\n",
    "Este notebook es parte de la serie [Introducción a las Redes Neuronales](https://multivacs.com/tags/intro-nn/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalación de dependencias\n",
    "\n",
    "Instalamos las librerías necesarias para ejecutar el código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install matplotlib numpy torch scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establecemos semilla\n",
    "\n",
    "Configuramos una semilla inicial para hacer que los resultados sean reproducibles. Esto es para todas las funciones que utilicen métodos aleatorios, tratar de obtener los mismos resultados independientemente de la ejecución.\n",
    "\n",
    "Digo tratar ya que aún así, es posible que torch utilice métodos no deterministas para la multiplicación de matrices, por lo que no se garantiza la reproducibilidad, aunque serán muy parecidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# establecemos esta variable de entorno previamente a importar pytorch para evitar operaciones no deterministas en GPU\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generamos datos sintéticos\n",
    "\n",
    "Vamos a generar un dataset de manera aleatoria un vector de dos elementos por cada clase, utilizando una distribución normal.\n",
    "\n",
    "El siguiente código genera datos para las dos clases utilizando `scikit-learn` y su función `sklearn.datasets.make_blobs`.  \n",
    "Esta función devuelve una tupla con los datos generados y la correspondiente etiqueta (en nuestro caso, 0 y 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class_centers = [[1,2], [3,4]]\n",
    "samples = 100  # número de muestras por clase\n",
    "features = 2  # dimensión de las muestras (num columnas)\n",
    "\n",
    "Xn, yn = make_blobs(n_samples=samples, centers=class_centers, cluster_std=1, n_features=features, random_state=42)\n",
    "\n",
    "print(f\"Xn.shape = {Xn.shape}\")\n",
    "print(f\"yn.shape = {yn.shape}\")\n",
    "print(f\"Xn[:3] = {Xn[:3]}\")\n",
    "print(f\"yn[:3] = {yn[:3]}\")\n",
    "\n",
    "plt.scatter(Xn[:,0], Xn[:,1], c=yn, cmap='tab20c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicializar tensores\n",
    "\n",
    "Como sabrás, en cuanto a computación, las CPUs están diseñadas para uso general, llevando a cabo tareas diversas y normalmente, de manera secuencial (en fila).  \n",
    "Mientras que las GPUs son hardware dedicado a una tarea muy específica, como es el procesamiento y cálculo en paralelo, haciéndolos muy eficientes para realizar tareas simultáneas como el entrenamiento de modelos de deep learning.\n",
    "\n",
    "Por tanto, vamos a pasar de vectores NumPy (CPU-efficient) a tensores PyTorch (GPU-optimized).  \n",
    "Para ello, utilizaremos la función nativa de PyTorch `torch.from_numpy`, que recoge como input un vector NumPy y lo transforma a un objeto de tipo `torch.Tensor`.\n",
    "\n",
    "Como dato adicional, NumPy utiliza doble precisión (FP64) por defecto, mientras que PyTorch usa 32 bits, ya que las GPUs son más rápidas en precisión simple y la diferencia en precisión es normalmente insignificante. Por eso, convertimos primero el vector NumPy de 64 a 32 bits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"  # comprobamos si tenemos gpu disponible\n",
    "print(f'Usando {device}')\n",
    "\n",
    "X = torch.from_numpy(np.float32(Xn)).to(device)\n",
    "y = torch.from_numpy(np.float32(yn)).to(device)\n",
    "\n",
    "print(f\"type(X) = {type(X)}, type(y) = {type(y)}\")\n",
    "print(f\"X[:3] = {X[:3]}\")\n",
    "print(f\"y[:3] = {y[:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividir los datos en entrenamiento y test\n",
    "\n",
    "En Machine Learning solemos diferenciar entre el conjunto de datos que utilizaremos para entrenar el modelo, y aquél usado para comprobar su rendimiento.  \n",
    "Esto es para poder ver cuál es la capacidad de generalización de nuestro modelo, ya que lo interesante no es que el modelo se aprenda todas las respuestas de nuestro listado de memoria (overfitting), sino que sea capaz de dada una entrada que previamente no ha visto, poder inferir la probabilidad de pertenencia a una clase.\n",
    "\n",
    "Adicionalmente, se crea un tercer subconjunto de los datos dedicado a validación. Para entender su función, digamos que es como una manera de ir comprobando periódicamente cada x iteraciones, si el modelo ha sido capaz de mejorar o no respecto a las iteraciones previas. Esto es para tratar de forzar que el modelo pare cuando haya dejado de mejorar en su entrenamiento y evitar el sobreentrenamiento.\n",
    "\n",
    "Para hacer la división crearemos una máscara poniendo a 0 aquellas muestras dedidacadas a entreno y a 1 las de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.ones(X.shape[0], dtype=bool).to(device)\n",
    "mask[::3] = 0  # cada 3 elementos lo utilizamos como test\n",
    "X_train, y_train = X[mask], y[mask]\n",
    "X_test, y_test = X[torch.logical_not(mask)], y[torch.logical_not(mask)]\n",
    "\n",
    "print(f\"X_train.shape = {X_train.shape}\")\n",
    "print(f\"y_train.shape = {y_train.shape}\")\n",
    "print(f\"X_test.shape = {X_test.shape}\")\n",
    "print(f\"y_test.shape = {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones auxiliares\n",
    "\n",
    "Definimos las siguientes funciones que serán utilizadas durante el entrenamiento e inferencia del modelo:\n",
    "\n",
    "- `regressor`: inicializa y devuelve los parámetros aprendibles (pesos y sesgo). Hay varias estrategias para inicializar estos parámetros, por ahora lo iniciaremos de manera aleatoria.\n",
    "- `sigmoid`: devuelve un tensor en el que cada elemento es la sigmoide del elemento correspondiente del tensor de entrada.\n",
    "- `forward`: devuelve la salida del regresor logístico.\n",
    "- `binary_cross_entropy`: devuelve la función de pérdida de entropía cruzada, dado el vector de predicciones y la correspondiente salida esperada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regressor(size):\n",
    "    weights = torch.rand(size, dtype=torch.float32).to(device)\n",
    "    bias = torch.rand(1, dtype=torch.float32).to(device)\n",
    "    return weights, bias\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + torch.exp(-x))\n",
    "\n",
    "def forward(X, weights, bias):\n",
    "    return sigmoid(torch.matmul(X,weights) + bias)\n",
    "\n",
    "def binary_cross_entropy(y_truth, y_pred):\n",
    "    m = 1 / y_truth.shape[0]  # y_truth.shape[0] es el tamaño del mini-batch\n",
    "    return -m * (y_truth * torch.log(y_pred) +\n",
    "                    (1 - y_truth) * torch.log(1 - y_pred)).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenando el modelo\n",
    "\n",
    "Estamos listos para entrenar nuestro modelo para hacer clasificación de dos clases. El proceso de entrenamiento lo establecemos en la función `train`, que recibe como entrada el mini-batch de datos, la salida esperada, los parámetros de peso y bias, el learning rate, número de pasos a realizar, y el número de pasos en los que se evalúa el modelo para propósitos de logging.\n",
    "\n",
    "La función devuelve el vector de pesos y bias aprendidos, y el error tras el último paso de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_backward(y_truth, y_pred, X):\n",
    "    '''\n",
    "    Calcula el gradiente.\n",
    "    '''\n",
    "    err = (y_pred - y_truth)\n",
    "    grad_w = (1 / y_truth.shape[0]) * torch.matmul(err, X)\n",
    "    grad_b = (1 / y_truth.shape[0]) * torch.sum(err)\n",
    "    return grad_w, grad_b\n",
    "\n",
    "def optimizer_step(weights, bias, grad_w, grad_b, lr=0.01):\n",
    "    '''\n",
    "    Actualiza los parámetros del modelo.\n",
    "    '''\n",
    "    weights = weights - lr * grad_w\n",
    "    bias = bias - lr * grad_b\n",
    "    return weights, bias\n",
    "\n",
    "def train(X, y_truth, weights, bias, lr=0.01, training_steps=1000, valid_steps=100):\n",
    "    for i in range(training_steps):\n",
    "        y_pred = forward(X, weights, bias)\n",
    "        grad_w, grad_b = loss_backward(y_truth, y_pred, X)\n",
    "        weights, bias = optimizer_step(weights, bias, grad_w, grad_b, lr)\n",
    "        if i % valid_steps == 0:\n",
    "            loss = binary_cross_entropy(y_truth, y_pred).item()  \n",
    "            # item() devuelve un escalar a partir de un tensor de un único valor \n",
    "            print (f'Step [{i}/{training_steps}], loss: {loss:.2f}')\n",
    "    loss = binary_cross_entropy(y_truth, y_pred).item()\n",
    "    print (f'Step [{training_steps}/{training_steps}], loss: {loss:.2f}')\n",
    "    return weights, bias, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.05\n",
    "training_steps = 100\n",
    "valid_steps = 10\n",
    "\n",
    "weights, bias = regressor(X_train.shape[1])\n",
    "weights, bias, bn_train = train(X_train, y_train, weights, bias,\n",
    "                            lr=learning_rate, training_steps=training_steps, valid_steps=valid_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluar el modelo (test)\n",
    "\n",
    "Una vez tenemos el modelo entrenado, con sus pesos y bias, usamos el conjunto de test para medir su rendimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Learned logistic regressor: y = σ({weights[0]:.2f}*x1 + {weights[1]:.2f}*x2 + {bias.item():.2f})')\n",
    "y_pred = forward(X_test, weights, bias)\n",
    "loss = binary_cross_entropy(y_test, y_pred).item()\n",
    "print(f'Binary cross-entropy on the test set: {loss:.2f}')\n",
    "prediction = y_pred > 0.5 \n",
    "correct = prediction == y_test\n",
    "accuracy = (torch.sum(correct) / y_test.shape[0])*100\n",
    "print (f'Accuracy on the test set: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizamos la frontera de decisión\n",
    "\n",
    "Por último dibujamos en una gráfica utilizando `matplotlib` la frontera de decisión del modelo, es decir, la línea (hiperplano) divisoria que separa las dos clases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = -weights[0].item() / weights[1].item()  # slope\n",
    "t = -bias.item() / weights[1].item()  # intercept\n",
    "\n",
    "plt.scatter(Xn[:,0], Xn[:,1], c=yn, cmap='tab20c')\n",
    "plt.title(f\"Hyperplane learned by the logistic regressor\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "x_hyperplane = np.linspace(0,6,100)\n",
    "y_hyperplane = r*x_hyperplane+t\n",
    "plt.plot(x_hyperplane, y_hyperplane, '-c')  # -c means solid cyan line\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pln",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
